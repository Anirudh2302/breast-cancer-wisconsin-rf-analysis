{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How Random Forest Hyperparameters Shape Decision Boundaries\n",
        "### Breast Cancer Wisconsin (Diagnostic) — Full tutorial notebook\n",
        "\n",
        "This notebook contains a complete pipeline you can run in Google Colab or Jupyter:\n",
        "- Load the **Breast Cancer** dataset (built-in sklearn version)\n",
        "- EDA (brief)\n",
        "- Random Forest feature importance and selecting top features\n",
        "- Decision boundaries using top-2 raw features\n",
        "- PCA (2 components) and decision boundary\n",
        "- NCA (2 components) and decision boundary\n",
        "- Compare methods and select the best\n",
        "- Study individual hyperparameters (max_depth, n_estimators, max_features)\n",
        "- GridSearchCV tuning and interpretation plots\n",
        "\n",
        "Each code cell includes student-friendly comments and short textual explanation for key points and keywords.\n",
        "\n",
        "Run the notebook from top to bottom. If using Colab, upload a copy of this notebook and run cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports and settings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "%matplotlib inline\n",
        "print('Libraries loaded.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load data and quick overview\n",
        "\n",
        "**Keywords:** dataset, features, target, shape\n",
        "\n",
        "We use `sklearn.datasets.load_breast_cancer` for a clean copy of the Breast Cancer dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "print('Feature count:', X.shape[1])\n",
        "print('Sample count:', X.shape[0])\n",
        "print('Target names:', target_names)\n",
        "\n",
        "# Put into a DataFrame for easy EDA\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Brief EDA (exploratory data analysis)\n",
        "\n",
        "Show distributions of a few top features and a correlation heatmap. **Keep EDA compact** for the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Simple distribution plots for three informative features\n",
        "sample_features = ['mean radius', 'mean texture', 'mean perimeter']\n",
        "for feat in sample_features:\n",
        "    plt.figure()\n",
        "    df.boxplot(column=feat, by='target')\n",
        "    plt.title(f'{feat} by class (0=benign,1=malignant)')\n",
        "    plt.suptitle('')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel(feat)\n",
        "    plt.show()\n",
        "\n",
        "# Correlation heatmap (small) - compute and plot\n",
        "corr = df.drop(columns=['target']).corr()\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(corr, interpolation='nearest')\n",
        "plt.title('Feature correlation matrix (visual)')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
        "plt.yticks(range(len(feature_names)), feature_names)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Random Forest Feature Importance (Gini) and Permutation Importance\n",
        "\n",
        "**Keywords:** feature importance, Gini importance, permutation importance, interpretability\n",
        "\n",
        "We train a Random Forest on the full feature set and extract feature importances. Then compute permutation importance for more robust ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Train/test split (full features)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train a baseline Random Forest\n",
        "rf_full = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_full.fit(X_train, y_train)\n",
        "\n",
        "# Gini-based feature importance\n",
        "gini_importances = rf_full.feature_importances_\n",
        "gini_idx = np.argsort(gini_importances)[::-1]\n",
        "print('Top 10 features by Gini importance:')\n",
        "for i in gini_idx[:10]:\n",
        "    print(f\"{feature_names[i]}: {gini_importances[i]:.4f}\")\n",
        "\n",
        "# Plot Gini importances\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(10), gini_importances[gini_idx[:10]][::-1])\n",
        "plt.yticks(range(10), [feature_names[i] for i in gini_idx[:10]][::-1])\n",
        "plt.xlabel('Gini importance')\n",
        "plt.title('Top 10 features by Gini importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Permutation importance (model-agnostic)\n",
        "perm = permutation_importance(rf_full, X_test, y_test, n_repeats=20, random_state=42, n_jobs=-1)\n",
        "perm_idx = np.argsort(perm.importances_mean)[::-1]\n",
        "print('\\nTop 10 features by permutation importance:')\n",
        "for i in perm_idx[:10]:\n",
        "    print(f\"{feature_names[i]}: mean imp={perm.importances_mean[i]:.4f}, std={perm.importances_std[i]:.4f}\")\n",
        "\n",
        "# Plot permutation importances\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(10), perm.importances_mean[perm_idx[:10]][::-1])\n",
        "plt.yticks(range(10), [feature_names[i] for i in perm_idx[:10]][::-1])\n",
        "plt.xlabel('Permutation importance (mean)')\n",
        "plt.title('Top 10 features by Permutation importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pick top 2 original features (from permutation importance) and plot decision boundary\n",
        "\n",
        "We use the top two features from permutation importance because they better reflect model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Select top 2 features according to permutation importance\n",
        "top2 = perm_idx[:2]\n",
        "top2_names = [feature_names[i] for i in top2]\n",
        "print('Top 2 features chosen for 2D plots:', top2_names)\n",
        "\n",
        "# Extract 2D data\n",
        "X2 = X[:, top2]\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "def plot_decision_boundary(model, X_plot, y_plot, title='Decision boundary'):\n",
        "    # Grid for plotting\n",
        "    x_min, x_max = X_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1\n",
        "    y_min, y_max = X_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X_plot[:,0], X_plot[:,1], c=y_plot, edgecolor='k')\n",
        "    plt.xlabel(top2_names[0])\n",
        "    plt.ylabel(top2_names[1])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Train RF on top2 and plot\n",
        "rf_top2 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_top2.fit(X2_train, y2_train)\n",
        "print('Accuracy on test (top2 raw features):', accuracy_score(y2_test, rf_top2.predict(X2_test)))\n",
        "plot_decision_boundary(rf_top2, X2_train, y2_train, title='Decision boundary (Top2 raw features)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PCA (2 components) — transform, train RF, plot boundary\n",
        "\n",
        "PCA is unsupervised and finds directions of maximum variance. We scale before PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "rf_pca = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_pca.fit(X_pca_train, y_pca_train)\n",
        "print('Accuracy on test (PCA 2 components):', accuracy_score(y_pca_test, rf_pca.predict(X_pca_test)))\n",
        "plot_decision_boundary(rf_pca, X_pca_train, y_pca_train, title='Decision boundary (PCA 2 components)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. NCA (2 components) — supervised projection, train RF, plot boundary\n",
        "\n",
        "NCA (Neighborhood Components Analysis) learns a projection that optimises nearest-neighbour classification. It is supervised and often produces better class separation than PCA for classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NCA requires scaled data\n",
        "scaler2 = StandardScaler()\n",
        "X_scaled2 = scaler2.fit_transform(X)\n",
        "nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)\n",
        "X_nca = nca.fit_transform(X_scaled2, y)\n",
        "X_nca_train, X_nca_test, y_nca_train, y_nca_test = train_test_split(X_nca, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "rf_nca = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_nca.fit(X_nca_train, y_nca_train)\n",
        "print('Accuracy on test (NCA 2 components):', accuracy_score(y_nca_test, rf_nca.predict(X_nca_test)))\n",
        "plot_decision_boundary(rf_nca, X_nca_train, y_nca_train, title='Decision boundary (NCA 2 components)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare methods and choose best\n",
        "\n",
        "Compare test accuracies and visual separation; choose the best for the final model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "acc_raw = accuracy_score(y2_test, rf_top2.predict(X2_test))\n",
        "acc_pca = accuracy_score(y_pca_test, rf_pca.predict(X_pca_test))\n",
        "acc_nca = accuracy_score(y_nca_test, rf_nca.predict(X_nca_test))\n",
        "print('Summary accuracies:')\n",
        "print('Raw top2:', acc_raw)\n",
        "print('PCA 2 comp:', acc_pca)\n",
        "print('NCA 2 comp:', acc_nca)\n",
        "\n",
        "methods = {'Raw':(acc_raw, 'raw'), 'PCA':(acc_pca, 'pca'), 'NCA':(acc_nca, 'nca')}\n",
        "best = max(methods.items(), key=lambda x: x[1][0])\n",
        "print('\\nBest method:', best[0])\n",
        "\n",
        "# Set final training/test accordingly\n",
        "if best[0] == 'Raw':\n",
        "    X_train_final, X_test_final = X2_train, X2_test\n",
        "    y_train_final, y_test_final = y2_train, y2_test\n",
        "elif best[0] == 'PCA':\n",
        "    X_train_final, X_test_final = X_pca_train, X_pca_test\n",
        "    y_train_final, y_test_final = y_pca_train, y_pca_test\n",
        "else:\n",
        "    X_train_final, X_test_final = X_nca_train, X_nca_test\n",
        "    y_train_final, y_test_final = y_nca_train, y_nca_test\n",
        "\n",
        "print('Chosen final feature set and shapes: ', X_train_final.shape, X_test_final.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Study individual hyperparameters effects\n",
        "\n",
        "We vary one hyperparameter at a time and plot accuracy vs value. This shows how each parameter affects performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "param_results = {}\n",
        "\n",
        "# max_depth\n",
        "depth_values = [2, 4, 6, 8, None]\n",
        "acc_depth = []\n",
        "for d in depth_values:\n",
        "    m = RandomForestClassifier(n_estimators=100, max_depth=d, random_state=42)\n",
        "    m.fit(X_train_final, y_train_final)\n",
        "    acc_depth.append(accuracy_score(y_test_final, m.predict(X_test_final)))\n",
        "param_results['max_depth'] = (depth_values, acc_depth)\n",
        "\n",
        "# n_estimators\n",
        "tree_values = [10, 50, 100, 200, 300]\n",
        "acc_trees = []\n",
        "for n in tree_values:\n",
        "    m = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    m.fit(X_train_final, y_train_final)\n",
        "    acc_trees.append(accuracy_score(y_test_final, m.predict(X_test_final)))\n",
        "param_results['n_estimators'] = (tree_values, acc_trees)\n",
        "\n",
        "# max_features\n",
        "feat_values = [1, 'sqrt', 'log2']\n",
        "acc_feat = []\n",
        "for f in feat_values:\n",
        "    m = RandomForestClassifier(max_features=f, random_state=42)\n",
        "    m.fit(X_train_final, y_train_final)\n",
        "    acc_feat.append(accuracy_score(y_test_final, m.predict(X_test_final)))\n",
        "param_results['max_features'] = (feat_values, acc_feat)\n",
        "\n",
        "# Plot results\n",
        "for name, (vals, accs) in param_results.items():\n",
        "    plt.figure()\n",
        "    # Convert vals to strings for plotting if they are not numeric\n",
        "    plt.plot(list(range(len(vals))), accs, marker='o')\n",
        "    plt.xticks(list(range(len(vals))), [str(v) for v in vals])\n",
        "    plt.xlabel(name)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Effect of {name} on accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Hyperparameter tuning: GridSearchCV\n",
        "\n",
        "We run GridSearchCV on a sensible grid and inspect the best parameters and CV scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [4, 6, 8, None],\n",
        "    'max_features': [1, 'sqrt', 'log2'],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train_final, y_train_final)\n",
        "print('Best CV score:', grid.best_score_)\n",
        "print('Best params:', grid.best_params_)\n",
        "\n",
        "# Simple plot of mean test scores (sorted)\n",
        "results = grid.cv_results_\n",
        "means = results['mean_test_score']\n",
        "stds = results['std_test_score']\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(means, marker='o')\n",
        "plt.title('GridSearchCV mean test scores (all combinations)')\n",
        "plt.xlabel('Parameter combination index')\n",
        "plt.ylabel('CV mean accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluate best model and final interpretation\n",
        "\n",
        "We show the confusion matrix, classification report, and final decision boundary (if 2D).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "best = grid.best_estimator_\n",
        "y_pred_final = best.predict(X_test_final)\n",
        "print('Final test accuracy:', accuracy_score(y_test_final, y_pred_final))\n",
        "print('\\nClassification report:\\n')\n",
        "print(classification_report(y_test_final, y_pred_final))\n",
        "cm = confusion_matrix(y_test_final, y_pred_final)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.title('Confusion Matrix - Final tuned model')\n",
        "plt.show()\n",
        "\n",
        "# If final data is 2D, plot decision boundary\n",
        "if X_train_final.shape[1] == 2:\n",
        "    plot_decision_boundary(best, X_train_final, y_train_final, title='Decision boundary - Final tuned RF')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key points and keywords (for your PDF)\n",
        "\n",
        "- **Random Forest**: ensemble of decision trees, reduces variance by averaging.\n",
        "- **max_depth**: deeper trees fit more complex patterns but risk overfitting.\n",
        "- **n_estimators**: more trees generally increase stability; diminishing returns after a point.\n",
        "- **max_features**: controls randomness and decorrelation among trees.\n",
        "- **min_samples_leaf**: regularises trees by requiring minimum samples in leaves.\n",
        "- **Permutation importance**: model-agnostic, measures drop in performance when a feature is shuffled.\n",
        "- **PCA vs NCA**: PCA is unsupervised variance-based projection; NCA is supervised and often better for classification separation.\n",
        "\n",
        "Include short interpretations under each plot in your PDF and relate observations back to bias–variance intuition.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}


